import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from rapidfuzz import fuzz
from google.oauth2 import service_account
from googleapiclient.discovery import build
import re
import gspread
from urllib.parse import urljoin, urlparse, urlparse

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏
st.set_page_config(
    page_title="–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤",
    page_icon="üìÑ",
    layout="wide"
)

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è session state
if 'data' not in st.session_state:
    st.session_state.data = None
if 'selected_row' not in st.session_state:
    st.session_state.selected_row = None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è Google Docs API –∫–ª—ñ—î–Ω—Ç–∞
@st.cache_resource
def get_docs_service():
    try:
        credentials = service_account.Credentials.from_service_account_info(
            st.secrets["gcp_service_account"],
            scopes=["https://www.googleapis.com/auth/documents.readonly"]
        )
        service = build('docs', 'v1', credentials=credentials)
        return service
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Google Docs API: {e}")
        return None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ Google Sheets
@st.cache_data(ttl=600)
def load_data_from_sheets():
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º gspread + service account –∏–∑ st.secrets
        creds = service_account.Credentials.from_service_account_info(
            st.secrets["gcp_service_account"],
            scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
        )

        client = gspread.Client(auth=creds)

        spreadsheet = st.secrets["connections"]["gsheets"]["spreadsheet"]
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ URL –∏–ª–∏ ID
        if isinstance(spreadsheet, str) and spreadsheet.startswith("http"):
            sh = client.open_by_url(spreadsheet)
        else:
            sh = client.open_by_key(spreadsheet)

        worksheet = sh.get_worksheet(0)
        
        # –ß–∏—Ç–∞—î–º–æ –≤—Å—ñ –¥–∞–Ω—ñ —è–∫ —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫—ñ–≤, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø–æ–º–∏–ª–∫–∏ –∑ –¥—É–±–ª—ñ–∫–∞—Ç–∞–º–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
        all_values = worksheet.get_all_values()
        
        if not all_values:
            return pd.DataFrame()

        # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame –∑ —É—Å—ñ–º–∞ –¥–∞–Ω–∏–º–∏
        df = pd.DataFrame(all_values)
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–µ—Ä—à–∏–π —Ä—è–¥–æ–∫ (–∑–∞–≥–æ–ª–æ–≤–∫–∏) —ñ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –π–æ–≥–æ –Ω–∞ —Å–ø–∏—Å–æ–∫
        header = df.iloc[0].tolist()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ —ñ–º–µ–Ω–∞ –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫
        new_columns = []
        col_counts = {}
        for i, col in enumerate(header):
            # –Ø–∫—â–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ—Ä–æ–∂–Ω—ñ–π, –¥–∞—î–º–æ –π–æ–º—É —Ç–∏–º—á–∞—Å–æ–≤–µ —ñ–º'—è
            if not col:
                col = f'Unnamed.{i}'
            # –Ø–∫—â–æ —Ç–∞–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∂–µ —î, –¥–æ–¥–∞—î–º–æ —Å—É—Ñ—ñ–∫—Å
            if col in col_counts:
                col_counts[col] += 1
                new_columns.append(f"{col}_{col_counts[col]}")
            else:
                col_counts[col] = 0
                new_columns.append(col)
        
        # –ü—Ä–∏–∑–Ω–∞—á–∞—î–º–æ –Ω–æ–≤—ñ, —É–Ω—ñ–∫–∞–ª—å–Ω—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        df.columns = new_columns
        
        # –í–∏–¥–∞–ª—è—î–º–æ –ø–µ—Ä—à–∏–π —Ä—è–¥–æ–∫, —è–∫–∏–π –±—É–≤ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
        df = df.iloc[1:].reset_index(drop=True)

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ usecols=list(range(10))
        if df.shape[1] > 10:
            df = df.iloc[:, :10]

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = df.dropna(subset=['–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∞–Ω–æ—Ç–∞—Ü—ñ—é, –∞–Ω–∫–æ—Ä', '–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å–∞–π—Ç—ñ'])
        df = df.reset_index(drop=True)

        return df
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")
        return None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è document ID –∑ URL
def extract_doc_id(url):
    if not url or not isinstance(url, str):
        return None
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Å—Å—ã–ª–∫–∏: /d/<id>/, ?id=<id>, –∏ —Ç.–ø.
    match = re.search(r'(?:/d/|/document/d/|[?&]id=)([-\w]+)', url)
    return match.group(1) if match else None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ Google Docs
@st.cache_data(ttl=3600)
def get_doc_text(doc_url):
    if not doc_url:
        return ""
    
    doc_id = extract_doc_id(doc_url)
    if not doc_id:
        return ""
    
    try:
        service = get_docs_service()
        if not service:
            return ""
        
        document = service.documents().get(documentId=doc_id).execute()
        
        # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
        content = document.get('body', {}).get('content', [])
        text_parts = []
        
        for element in content:
            if 'paragraph' in element:
                paragraph = element['paragraph']
                for text_run in paragraph.get('elements', []):
                    if 'textRun' in text_run:
                        text_parts.append(text_run['textRun']['content'])
            elif 'table' in element:
                table = element['table']
                for row in table.get('tableRows', []):
                    for cell in row.get('tableCells', []):
                        for cell_content in cell.get('content', []):
                            if 'paragraph' in cell_content:
                                for text_run in cell_content['paragraph'].get('elements', []):
                                    if 'textRun' in text_run:
                                        text_parts.append(text_run['textRun']['content'])
        
        text = ''.join(text_parts)
        # –û—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    except Exception as e:
        st.warning(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {e}")
        return ""

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ä–µ–¥–∞–∫—Ç–æ—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ
def check_editors_on_page(page_text):
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ö–æ—á–∞ –± –æ–¥–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ
    –†–µ–¥–∞–∫—Ç–æ—Ä–∏: "–©–µ—Ä–±–∞—á–µ–Ω–∫–æ –Æ–ª—ñ—è" –∞–±–æ "–°–µ–≤—Ä—é–∫–æ–≤ –û–ª–µ–∫—Å–∞–Ω–¥—Ä –í—ñ–∫—Ç–æ—Ä–æ–≤–∏—á"
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö –∑–Ω–∞–π–¥–µ–Ω–∏—Ö —Ä–µ–¥–∞–∫—Ç–æ—Ä—ñ–≤
    """
    if not page_text:
        return False, []
    
    editors = [
        "–©–µ—Ä–±–∞—á–µ–Ω–∫–æ –Æ–ª—ñ—è",
        "–°–µ–≤—Ä—é–∫–æ–≤ –û–ª–µ–∫—Å–∞–Ω–¥—Ä –í—ñ–∫—Ç–æ—Ä–æ–≤–∏—á"
    ]
    
    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ—à—É–∫—É (–∑–∞–º—ñ–Ω—é—î–º–æ –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±—ñ–ª–∏)
    normalized_text = page_text.replace('\xa0', ' ')
    
    found_editors = []
    for editor in editors:
        # –®—É–∫–∞—î–º–æ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞ –≤ —Ç–µ–∫—Å—Ç—ñ (—ñ–≥–Ω–æ—Ä—É—é—á–∏ —Ä–µ–≥—ñ—Å—Ç—Ä)
        if re.search(re.escape(editor), normalized_text, re.IGNORECASE):
            found_editors.append(editor)
    
    return len(found_editors) > 0, found_editors

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –±–ª–æ–∫—É "–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã" —Ç–∞ –ø–æ—Å–∏–ª–∞–Ω—å –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Å–∞–π—Ç—É
@st.cache_data(ttl=3600)
def extract_references_section(page_url):
    """
    –í–∏—Ç—è–≥—É—î –±–ª–æ–∫ '–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã' –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Å–∞–π—Ç—É —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –ø–æ—Å–∏–ª–∞–Ω—å
    –û–±–º–µ–∂—É—î –ø–æ—à—É–∫ —Ç—ñ–ª—å–∫–∏ –±–ª–æ–∫–æ–º –ø—ñ—Å–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –≤–µ–ª–∏–∫–æ–≥–æ —Ä–æ–∑–¥—ñ–ª—É
    """
    if not page_url:
        return []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –¥–æ–º–µ–Ω –ø–æ—Ç–æ—á–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ—Ö –ø–æ—Å–∏–ª–∞–Ω—å
        page_domain = None
        try:
            parsed = urlparse(page_url)
            page_domain = parsed.netloc.lower()
        except:
            pass
        
        # –í–∞—Ä—ñ–∞–Ω—Ç–∏ –Ω–∞–∑–≤ —Ä–æ–∑–¥—ñ–ª—É
        section_patterns = [
            r'–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã',
            r'–°–ø–∏—Å–æ–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ—ó –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏',
            r'–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã',
            r'–°–ø–∏—Å–æ–∫ –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏',
            r'–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',
            r'–õ—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∞',
            r'References',
            r'Bibliography'
        ]
        
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–æ–∑–¥—ñ–ª—É
        section_header = None
        
        # –°–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î–º–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö (h1-h6)
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = tag.get_text(strip=True)
            if not text:
                continue
                
            text_lower = text.lower()
            for pattern in section_patterns:
                if re.search(pattern, text_lower, re.IGNORECASE):
                    section_header = tag
                    break
            
            if section_header:
                break
        
        # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö, —à—É–∫–∞—î–º–æ –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞—Ö —Ç–∞ div (–∫–æ—Ä–æ—Ç–∫—ñ —Ç–µ–∫—Å—Ç–∏)
        if not section_header:
            for tag in soup.find_all(['p', 'div', 'section', 'span']):
                text = tag.get_text(strip=True)
                if not text or len(text) > 100:  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–∑–≤–∏—á–∞–π –∫–æ—Ä–æ—Ç–∫–∏–π
                    continue
                    
                text_lower = text.lower()
                for pattern in section_patterns:
                    if re.search(pattern, text_lower, re.IGNORECASE):
                        section_header = tag
                        break
                
                if section_header:
                    break
        
        references_links = []
        
        if section_header:
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞—Å—Ç—É–ø–Ω—ñ sibling –µ–ª–µ–º–µ–Ω—Ç–∏ –ø—ñ—Å–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            # –û–±–º–µ–∂—É—î–º–æ –ø–æ—à—É–∫ –º–∞–∫—Å–∏–º—É–º 20 –µ–ª–µ–º–µ–Ω—Ç–∞–º–∏ –ø—ñ—Å–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            section_content = []
            count = 0
            max_elements = 20
            
            # –®—É–∫–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ –ø—ñ—Å–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            current = section_header.next_sibling
            while current and count < max_elements:
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ —Ç–µ–≥ (–Ω–µ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤–∏–π –≤—É–∑–æ–ª)
                if hasattr(current, 'name') and current.name:
                    # –ó—É–ø–∏–Ω—è—î–º–æ—Å—è –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É –≤–µ–ª–∏–∫–æ–º—É –∑–∞–≥–æ–ª–æ–≤–∫—É
                    if current.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                        break
                    # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –µ–ª–µ–º–µ–Ω—Ç–∏, —â–æ –º–æ–∂—É—Ç—å –º—ñ—Å—Ç–∏—Ç–∏ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                    if current.name in ['p', 'div', 'li', 'ul', 'ol', 'span', 'a', 'section']:
                        section_content.append(current)
                        count += 1
                current = current.next_sibling
            
            # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ —á–µ—Ä–µ–∑ siblings, —à—É–∫–∞—î–º–æ –≤ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–æ–º—É –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ñ
            if len(section_content) < 3:
                parent_container = section_header.find_parent(['div', 'section', 'article', 'main'])
                if parent_container:
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ñ –ø—ñ—Å–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    found_header = False
                    count = 0
                    
                    for elem in parent_container.find_all(['p', 'div', 'li', 'span', 'a', 'ul', 'ol']):
                        if elem == section_header:
                            found_header = True
                            continue
                        
                        if found_header and count < max_elements:
                            # –ó—É–ø–∏–Ω—è—î–º–æ—Å—è –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–æ–º—É –≤–µ–ª–∏–∫–æ–º—É –∑–∞–≥–æ–ª–æ–≤–∫—É
                            if elem.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                                break
                            if elem not in section_content:
                                section_content.append(elem)
                                count += 1
            
            # –í–∏—Ç—è–≥—É—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ –∑—ñ –∑–Ω–∞–π–¥–µ–Ω–æ–≥–æ –±–ª–æ–∫—É
            for elem in section_content:
                # –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –≤ —Ç–µ–≥–∞—Ö <a>
                for link in elem.find_all('a', href=True):
                    href = link.get('href', '')
                    if href and not href.startswith('#') and not href.startswith('javascript:'):
                        if href.startswith('http://') or href.startswith('https://'):
                            references_links.append(href)
                        elif href.startswith('/'):
                            full_url = urljoin(page_url, href)
                            references_links.append(full_url)
                        else:
                            full_url = urljoin(page_url, href)
                            references_links.append(full_url)
                
                # –®—É–∫–∞—î–º–æ URL –≤ —Ç–µ–∫—Å—Ç—ñ (—Ñ–æ—Ä–º–∞—Ç "—Ç–µ–∫—Å—Ç / domain.com" –∞–±–æ –ø–æ–≤–Ω–∏–π URL)
                elem_text = elem.get_text(separator='\n')
                
                # –†–æ–∑–±–∏–≤–∞—î–º–æ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—è–¥–∫–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Ñ–æ—Ä–º–∞—Ç—É "—Ç–µ–∫—Å—Ç / domain.com"
                lines = elem_text.split('\n')
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # –ü–∞—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–≤–Ω–∏—Ö URL
                    full_url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?]'
                    found_urls = re.findall(full_url_pattern, line)
                    references_links.extend(found_urls)
                    
                    # –ü–∞—Ç–µ—Ä–Ω –¥–ª—è –¥–æ–º–µ–Ω—ñ–≤ —É —Ñ–æ—Ä–º–∞—Ç—ñ "—Ç–µ–∫—Å—Ç / domain.com" –∞–±–æ –ø—Ä–æ—Å—Ç–æ "domain.com"
                    # –®—É–∫–∞—î–º–æ –ø–∞—Ç–µ—Ä–Ω —Ç–∏–ø—É "—Ç–µ–∫—Å—Ç / domain.com" –∞–±–æ –ø—Ä–æ—Å—Ç–æ "domain.com"
                    # –ê–ª–µ —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —Ü–µ –Ω–µ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ–π –¥–æ–º–µ–Ω
                    # –ü–∞—Ç–µ—Ä–Ω –¥–ª—è —Ñ–æ—Ä–º–∞—Ç—É "—Ç–µ–∫—Å—Ç / domain.com" –∞–±–æ "domain.com;"
                    domain_pattern = r'([a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]*\.[a-zA-Z]{2,}(?:/[^\s<>"{}|\\^`\[\]]*)?)'
                    found_domains = re.findall(domain_pattern, line)
                    for domain in found_domains:
                        domain = domain.strip(' /;.,')
                        # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ, —è–∫—â–æ —Ü–µ –≤–∂–µ –ø–æ–≤–Ω–∏–π URL
                        if not domain.startswith('http'):
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ –Ω–µ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ–π –¥–æ–º–µ–Ω
                            try:
                                domain_netloc = urlparse('https://' + domain).netloc.lower()
                                if page_domain and (domain_netloc == page_domain or domain_netloc.endswith('.' + page_domain)):
                                    continue
                            except:
                                pass
                            # –î–æ–¥–∞—î–º–æ –ø—Ä–æ—Ç–æ–∫–æ–ª
                            full_url = 'https://' + domain
                            references_links.append(full_url)
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è - –≤–∏–¥–∞–ª—è—î–º–æ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —Å–∞–π—Ç—É —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–æ –¥–æ–º–µ–Ω—ñ–≤
        filtered_domains = set()  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ set –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        
        for link in references_links:
            try:
                # –ü–∞—Ä—Å–∏–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                link_parsed = urlparse(link)
                link_domain = link_parsed.netloc.lower()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ –∞–±–æ –Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                if not link_domain:
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                if page_domain:
                    if link_domain == page_domain or link_domain.endswith('.' + page_domain):
                        continue
                
                # –í–∏–¥–∞–ª—è—î–º–æ www. –∑ –¥–æ–º–µ–Ω—É
                if link_domain.startswith('www.'):
                    link_domain = link_domain[4:]
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ –∑ –¥–æ–º–µ–Ω–æ–º
                normalized_link = 'https://' + link_domain
                filtered_domains.add(normalized_link)
                
            except Exception:
                continue
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ set –≤ —Å–ø–∏—Å–æ–∫ —Ç–∞ —Å–æ—Ä—Ç—É—î–º–æ
        unique_links = sorted(list(filtered_domains))
        
        return unique_links
    
    except Exception as e:
        st.warning(f"–ü–æ–º–∏–ª–∫–∞ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –ª—ñ—Ç–µ—Ä–∞—Ç—É—Ä–∏ –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page_url}: {e}")
        return []

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
@st.cache_data(ttl=3600)
def get_page_text(page_url):
    if not page_url:
        return ""
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å–∫—Ä–∏–ø—Ç—ñ–≤ —Ç–∞ —Å—Ç–∏–ª—ñ–≤
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        text = soup.get_text(separator='\n')
        # –û—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
        text = text.lower()
        text = text.replace('\xa0', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    except Exception as e:
        st.warning(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page_url}: {e}")
        return ""





# –í–∏—Ç—è–≥–Ω—É—Ç–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –º—ñ–∂ –º–∞—Ä–∫–µ—Ä–∞–º–∏ '–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è' —ñ '–ê–Ω–∫–æ—Ä'
def extract_annotation_fragment(text: str) -> str:
    """
    –í–∏—Ç—è–≥—É—î —Ñ—Ä–∞–≥–º–µ–Ω—Ç "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è" –∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ä–æ—Å—ñ–π—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
    """
    if not text or not isinstance(text, str):
        return ""

    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –º–∞—Ä–∫–µ—Ä–∏ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –ø–æ—à—É–∫—É (—è–∫ —É —É–∫—Ä–∞—ó–Ω—Å—å–∫—ñ–π –≤–µ—Ä—Å—ñ—ó)
    t = text.replace('\xa0', ' ')
    t = re.sub(r'\s+', ' ', t)

    # –í–∞—Ä—ñ–∞–Ω—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ (–∫–∏—Ä–∏–ª–∏—Ü—è): –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è / –ê–Ω–Ω–æ—Ç–∞—Ü—ñ—è
    # –í–∞–∂–ª–∏–≤–æ: —à—É–∫–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è" –ë–ï–ó "–ü–µ—Ä–µ–≤–æ–¥" –ø–µ—Ä–µ–¥ –Ω–∏–º
    start_patterns = [r'–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è', r'–ê–Ω–Ω–æ—Ç–∞—Ü—ñ—è']
    # –ö—ñ–Ω—Ü–µ–≤–∏–π –º–∞—Ä–∫–µ—Ä: —Å–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î–º–æ "–ê–Ω–∫–æ—Ä", –ø–æ—Ç—ñ–º "–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏" —è–∫ —Ä–µ–∑–µ—Ä–≤
    end_patterns_primary = [r'–ê–Ω–∫–æ—Ä', r'Anchor', r'–ê–Ω–∫o—Ä']
    end_patterns_secondary = [r'–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏', r'–ü–µ—Ä–µ–∫–ª–∞–¥ –∞–Ω–Ω–æ—Ç–∞—Ü—ñ—ó', r'–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü—ñ—ó', r'–ü–µ—Ä–µ–∫–ª–∞–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏']

    # –ü–æ–±—É–¥—É—î–º–æ regex –¥–ª—è –ø–æ—à—É–∫—É –ø–æ–∑–∏—Ü—ñ–π (—ñ–≥–Ω–æ—Ä—É—é—á–∏ —Ä–µ–≥—ñ—Å—Ç—Ä —ñ –º–æ–∂–ª–∏–≤—ñ –¥–≤–æ–∫—Ä–∞–ø–∫–∏/–∑—ñ—Ä–æ—á–∫–∏)
    # –®—É–∫–∞—î–º–æ –≤—Å—ñ –≤—Ö–æ–¥–∂–µ–Ω–Ω—è "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è", –∞ –ø–æ—Ç—ñ–º –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–µ—Ä–µ–¥ –Ω–∏–º –Ω–µ–º–∞—î "–ü–µ—Ä–µ–≤–æ–¥"
    start_re = re.compile(r'(' + '|'.join(start_patterns) + r')\s*[:\-‚Äì‚Äî]*', flags=re.IGNORECASE)
    
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –º–æ–∂–ª–∏–≤—ñ –≤—Ö–æ–¥–∂–µ–Ω–Ω—è
    all_matches = list(start_re.finditer(t))
    
    # –®—É–∫–∞—î–º–æ –ø–µ—Ä—à–µ –≤—Ö–æ–¥–∂–µ–Ω–Ω—è, –ø–µ—Ä–µ–¥ —è–∫–∏–º –ù–ï —Å—Ç–æ—ó—Ç—å "–ü–µ—Ä–µ–≤–æ–¥" –∞–±–æ "–ü–µ—Ä–µ–∫–ª–∞–¥"
    start_match = None
    for match in all_matches:
        start_idx = match.start()
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø–µ—Ä–µ–¥ "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è" –Ω–µ–º–∞—î "–ü–µ—Ä–µ–≤–æ–¥" –∞–±–æ "–ü–µ—Ä–µ–∫–ª–∞–¥"
        before_text = t[max(0, start_idx-20):start_idx].lower()
        if '–ø–µ—Ä–µ–≤–æ–¥' not in before_text and '–ø–µ—Ä–µ–∫–ª–∞–¥' not in before_text:
            start_match = match
            break
    
    if not start_match:
        # –Ø–∫—â–æ –º–∞—Ä–∫–µ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç (—è–∫ –±—É–ª–æ —Ä–∞–Ω—ñ—à–µ)
        return t.strip()

    # –®—É–∫–∞—î–º–æ –ø–µ—Ä—à–∏–π –º–∞—Ç—á end –ø—ñ—Å–ª—è start
    start_pos = start_match.end()
    
    # –°–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω–∏–π –∫—ñ–Ω—Ü–µ–≤–∏–π –º–∞—Ä–∫–µ—Ä "–ê–Ω–∫–æ—Ä"
    end_re_primary = re.compile(r'(' + '|'.join(end_patterns_primary) + r')\s*[:\-‚Äì‚Äî]*', flags=re.IGNORECASE)
    end_match = end_re_primary.search(t, pos=start_pos)
    
    # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ "–ê–Ω–∫–æ—Ä", —à—É–∫–∞—î–º–æ "–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏" —è–∫ —Ä–µ–∑–µ—Ä–≤
    if not end_match:
        end_re_secondary = re.compile(r'(' + '|'.join(end_patterns_secondary) + r')\s*[:\-‚Äì‚Äî]*', flags=re.IGNORECASE)
        end_match = end_re_secondary.search(t, pos=start_pos)

    if end_match:
        fragment = t[start_pos:end_match.start()]
    else:
        # –Ø–∫—â–æ –µ–Ω–¥ –º–∞—Ä–∫–µ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî –±–µ—Ä–µ–º–æ —Ä–µ—à—Ç—É —Ç–µ–∫—Å—Ç—É –ø—ñ—Å–ª—è –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        fragment = t[start_pos:]

    # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ "—Å–∏—Ä–∏–π" —Ñ—Ä–∞–≥–º–µ–Ω—Ç, –æ—á–∏—â–µ–Ω–Ω—è –±—É–¥–µ –≤ compare_texts
    return fragment.strip()

# –í–∏—Ç—è–≥–Ω—É—Ç–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç "–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏" –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
def extract_ukrainian_annotation_fragment(text: str) -> str:
    """
    –í–∏—Ç—è–≥—É—î —Ñ—Ä–∞–≥–º–µ–Ω—Ç "–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏" –∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
    """
    if not text or not isinstance(text, str):
        return ""

    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –º–∞—Ä–∫–µ—Ä–∏ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –ø–æ—à—É–∫—É
    t = text.replace('\xa0', ' ')
    t = re.sub(r'\s+', ' ', t)

    # –í–∞—Ä—ñ–∞–Ω—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –≤–µ—Ä—Å—ñ—ó
    start_patterns = [r'–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏', r'–ü–µ—Ä–µ–∫–ª–∞–¥ –∞–Ω–Ω–æ—Ç–∞—Ü—ñ—ó', r'–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü—ñ—ó', r'–ü–µ—Ä–µ–∫–ª–∞–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏']
    # –ö—ñ–Ω—Ü–µ–≤–∏–π –º–∞—Ä–∫–µ—Ä –º–æ–∂–µ –±—É—Ç–∏ "–ê–Ω–∫–æ—Ä" –∞–±–æ –∫—ñ–Ω–µ—Ü—å –¥–æ–∫—É–º–µ–Ω—Ç–∞
    end_patterns = [r'–ê–Ω–∫–æ—Ä', r'Anchor', r'–ê–Ω–∫o—Ä']

    # –ü–æ–±—É–¥—É—î–º–æ regex –¥–ª—è –ø–æ—à—É–∫—É –ø–æ–∑–∏—Ü—ñ–π (—ñ–≥–Ω–æ—Ä—É—é—á–∏ —Ä–µ–≥—ñ—Å—Ç—Ä —ñ –º–æ–∂–ª–∏–≤—ñ –¥–≤–æ–∫—Ä–∞–ø–∫–∏/–∑—ñ—Ä–æ—á–∫–∏)
    start_re = re.compile(r'(' + '|'.join(start_patterns) + r')\s*[:\-‚Äì‚Äî]*', flags=re.IGNORECASE)
    end_re = re.compile(r'(' + '|'.join(end_patterns) + r')\s*[:\-‚Äì‚Äî]*', flags=re.IGNORECASE)

    start_match = start_re.search(t)
    if not start_match:
        return ""

    # –®—É–∫–∞—î–º–æ –ø–µ—Ä—à–∏–π –º–∞—Ç—á end –ø—ñ—Å–ª—è start
    start_pos = start_match.end()
    end_match = end_re.search(t, pos=start_pos)

    if end_match:
        fragment = t[start_pos:end_match.start()]
    else:
        # –Ø–∫—â–æ –µ–Ω–¥ –º–∞—Ä–∫–µ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî –±–µ—Ä–µ–º–æ —Ä–µ—à—Ç—É —Ç–µ–∫—Å—Ç—É –ø—ñ—Å–ª—è "–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"
        fragment = t[start_pos:]

    # –û—á–∏—Å—Ç–∏–º–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤—ñ–¥ –∑–∞–π–≤–∏—Ö –∑—ñ—Ä–æ—á–æ–∫/–º–µ—Ç–æ–∫ —ñ –æ–±—Ä—ñ–∂–µ–º–æ
    fragment = re.sub(r'[\*_#]{1,}', ' ', fragment)
    fragment = re.sub(r'\s+', ' ', fragment).strip()

    return fragment

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó URL –∑ —Ä–æ—Å—ñ–π—Å—å–∫–æ—ó
def generate_ukrainian_url(ru_url: str) -> str:
    """
    –ì–µ–Ω–µ—Ä—É—î —É–∫—Ä–∞—ó–Ω—Å—å–∫—É URL –∑ —Ä–æ—Å—ñ–π—Å—å–∫–æ—ó, –¥–æ–¥–∞—é—á–∏ /ua/ –ø–µ—Ä–µ–¥ —à–ª—è—Ö–æ–º
    –ü—Ä–∏–∫–ª–∞–¥: https://apteka911.ua/shop/... -> https://apteka911.ua/ua/shop/...
    """
    if not ru_url or not isinstance(ru_url, str):
        return ""
    
    try:
        parsed = urlparse(ru_url)
        path = parsed.path
        
        # –Ø–∫—â–æ URL –≤–∂–µ –º—ñ—Å—Ç–∏—Ç—å /ua/, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —è–∫ —î
        if '/ua/' in path:
            return ru_url
        
        # –î–æ–¥–∞—î–º–æ /ua/ –ø–µ—Ä–µ–¥ —à–ª—è—Ö–æ–º
        if path.startswith('/'):
            new_path = '/ua' + path
        else:
            new_path = '/ua/' + path
        
        # –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤—É URL
        new_url = f"{parsed.scheme}://{parsed.netloc}{new_path}"
        if parsed.query:
            new_url += f"?{parsed.query}"
        if parsed.fragment:
            new_url += f"#{parsed.fragment}"
        
        return new_url
    except Exception as e:
        return ""

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ä–æ—Å—ñ–π—Å—å–∫–æ—ó URL –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó
def generate_russian_url(url: str) -> str:
    """
    –ì–µ–Ω–µ—Ä—É—î —Ä–æ—Å—ñ–π—Å—å–∫—É URL –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó, –≤–∏–¥–∞–ª—è—é—á–∏ /ua/ –∑—ñ —à–ª—è—Ö—É.
    –ü—Ä–∏–∫–ª–∞–¥: https://apteka911.ua/ua/shop/... -> https://apteka911.ua/shop/...
    """
    if not url or not isinstance(url, str):
        return ""
    
    try:
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ urlparse –¥–ª—è –Ω–∞–¥—ñ–π–Ω–æ—ó –º–∞–Ω—ñ–ø—É–ª—è—Ü—ñ—ó —à–ª—è—Ö–æ–º
        parsed = urlparse(url)
        path = parsed.path
        
        new_path = path
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —à–ª—è—Ö –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ /ua/ –∞–±–æ –¥–æ—Ä—ñ–≤–Ω—é—î /ua
        if path.startswith('/ua/'):
            new_path = path[3:]  # –í–∏–¥–∞–ª—è—î–º–æ /ua, –∑–∞–ª–∏—à–∞—é—á–∏ / –Ω–∞ –ø–æ—á–∞—Ç–∫—É
        elif path == '/ua':
            new_path = '/' # –Ø–∫—â–æ —à–ª—è—Ö –±—É–≤ –ø—Ä–æ—Å—Ç–æ /ua, —Ä–æ–±–∏–º–æ –π–æ–≥–æ –∫–æ—Ä–µ–Ω–µ–≤–∏–º
        
        # –ó–±–∏—Ä–∞—î–º–æ URL –Ω–∞–∑–∞–¥
        from urllib.parse import urlunparse
        new_url_parts = list(parsed)
        new_url_parts[2] = new_path
        return urlunparse(new_url_parts)
        
    except Exception:
        return ""

# –°–ø—Ä–æ—â–µ–Ω–µ –æ—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è (–±–µ–∑ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –≤–∞–∂–ª–∏–≤–∏—Ö —á–∞—Å—Ç–∏–Ω)
def clean_text_for_comparison(text: str) -> str:
    """
    –°–ø—Ä–æ—â–µ–Ω–µ –æ—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è, –±–µ–∑ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –≤–∞–∂–ª–∏–≤–∏—Ö —á–∞—Å—Ç–∏–Ω
    """
    if not text or not isinstance(text, str):
        return ""
    
    # Replace non-breaking spaces and normalize newlines/spaces
    t = text.replace('\xa0', ' ')
    t = t.replace('\r\n', '\n').replace('\r', '\n')
    
    # Remove HTML tags
    t = re.sub(r'<[^>]+>', ' ', t)
    
    # Remove bold/italic markers like **bold**, __italic__, *italic*
    t = re.sub(r'(\*\*|__)(.*?)\1', r'\2', t)
    t = re.sub(r'(?<!\*)\*(?!\*)', ' ', t)
    t = re.sub(r'_(.*?)_', r'\1', t)
    
    # Remove markdown headings (# Heading)
    t = re.sub(r'(?m)^\s{0,3}#+\s*', ' ', t)
    
    # Remove footnote markers like [1], [^1]
    t = re.sub(r'\[\^?.*?\]', ' ', t)
    
    # Remove multiple punctuation sequences
    t = re.sub(r'[\-\u2014]{2,}', ' ', t)
    t = re.sub(r'\.\.{2,}', ' ', t)
    
    # Remove leftover asterisks and underscores
    t = t.replace('*', ' ').replace('_', ' ')
    
    # Collapse whitespace and trim
    t = re.sub(r'\s+', ' ', t).strip()
    
    return t

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤
def compare_texts(doc_fragment, page_text, threshold=75, chunk_size=300):
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î —Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑ —Ç–µ–∫—Å—Ç–æ–º —Å—Ç–æ—Ä—ñ–Ω–∫–∏
    doc_fragment - –≤–∂–µ –≤–∏—Ç—è–≥–Ω—É—Ç–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç (–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –∞–±–æ –ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏)
    """
    if not doc_fragment or not page_text:
        return 0.0, 0.0, []

    # –û—á–∏—â–∞—î–º–æ –≤–∏—Ç—è–≥–Ω—É—Ç–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —ñ —Ç–µ–∫—Å—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –ø–µ—Ä–µ–¥ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è–º
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–ø—Ä–æ—â–µ–Ω–µ –æ—á–∏—â–µ–Ω–Ω—è, —â–æ–± –Ω–µ –≤–∏–¥–∞–ª—è—Ç–∏ –≤–∞–∂–ª–∏–≤—ñ —á–∞—Å—Ç–∏–Ω–∏
    doc_text_clean = clean_text_for_comparison(doc_fragment)
    page_text_clean = clean_text_for_comparison(page_text)

    doc_text_lower = doc_text_clean.lower()
    page_text_lower = page_text_clean.lower()

    # –†–æ–∑–±–∏—Ç—Ç—è –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ (–ø–æ —Å–ª–æ–≤–∞—Ö) –∑ –Ω–µ–≤–µ–ª–∏–∫–∏–º –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è–º
    words = doc_text_lower.split()
    if not words:
        return 0.0, 0.0, []

    chunks = []
    # –î–µ–ª–∞—é—Ç —à–∞–≥ —Ä–∞–≤–Ω—ã–π 80% —Ä–∞–∑–º–µ—Ä–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ -> 20% overlap
    step = max(1, int(chunk_size * 0.8))

    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—á–∏—â–µ–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±–æ—Ö —Å–ø–∏—Å–∫—ñ–≤, —â–æ–± —ñ–Ω–¥–µ–∫—Å–∏ —Å–ø—ñ–≤–ø–∞–¥–∞–ª–∏
    orig_words_clean = doc_text_clean.split()
    min_words_threshold = 10

    for i in range(0, len(words), step):
        chunk_words = words[i:i + chunk_size]
        if len(chunk_words) >= min_words_threshold:
            # –ë–µ—Ä–µ–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ —Å–ª–æ–≤–∞ –∑ –æ—á–∏—â–µ–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ (–¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è)
            orig_chunk_words = orig_words_clean[i:i + chunk_size] if i < len(orig_words_clean) else []
            original = ' '.join(orig_chunk_words) if orig_chunk_words else ' '.join(chunk_words)
            normalized = ' '.join(chunk_words)
            chunks.append({
                'original': original,
                'normalized': normalized
            })

    if not chunks:
        return 0.0, 0.0, []

    # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Å—Ö–æ–∂–æ—Å—Ç—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
    scores = []
    missing_fragments = []

    for chunk in chunks:
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ token_set_ratio –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        score = fuzz.token_set_ratio(chunk['normalized'], page_text_lower)
        scores.append(score)

        if score < threshold:
            missing_fragments.append({
                'text': chunk['original'][:200] + '...' if len(chunk['original']) > 200 else chunk['original'],
                'score': score
            })

    min_score = min(scores) if scores else 0.0
    avg_score = sum(scores) / len(scores) if scores else 0.0

    return min_score, avg_score, missing_fragments

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.title("üìÑ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤ –∑ Google Docs")
st.markdown("---")

# –ë—ñ—á–Ω–∞ –ø–∞–Ω–µ–ª—å –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏
with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è")
    
    threshold = st.slider(
        "–ü–æ—Ä—ñ–≥ –ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ (%)",
        min_value=0,
        max_value=100,
        value=75,
        help="–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –≤—ñ–¥—Å–æ—Ç–æ–∫ —Å—Ö–æ–∂–æ—Å—Ç—ñ –¥–ª—è –≤–≤–∞–∂–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É —Ä–æ–∑–º—ñ—â–µ–Ω–∏–º"
    )
    
    chunk_size = st.slider(
        "–†–æ–∑–º—ñ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ (—Å–ª–æ–≤–∞)",
        min_value=100,
        max_value=500,
        value=300,
        step=50,
        help="–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤ —É –∫–æ–∂–Ω–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏"
    )
    
    show_problems_only = st.checkbox(
        "–ü–æ–∫–∞–∑–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ñ –∑–∞–ø–∏—Å–∏",
        value=False
    )
    
    st.markdown("---")
    
    # –ö–Ω–æ–ø–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
    if st.button("üîÑ –û–Ω–æ–≤–∏—Ç–∏ –¥–∞–Ω—ñ", use_container_width=True):
        st.cache_data.clear()
        st.session_state.data = None
        st.rerun()
    
    st.markdown("---")
    st.markdown("### üìä –õ–µ–≥–µ–Ω–¥–∞")
    st.markdown(f"""
    - üü¢ **–î–æ–±—Ä–µ**: ‚â• {threshold}%
    - üü° **–£–≤–∞–≥–∞**: {threshold-15}‚Äì{threshold-1}%
    - üî¥ **–ü—Ä–æ–±–ª–µ–º–∞**: < {threshold-15}%
    """)

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
if st.session_state.data is None:
    with st.spinner("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ Google Sheets..."):
        st.session_state.data = load_data_from_sheets()

df = st.session_state.data

if df is not None and not df.empty:
    st.success(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤")
    
    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫—É –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –≤—Å—ñ—Ö –∑–∞–ø–∏—Å—ñ–≤", type="primary", use_container_width=True):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            results = []
            for idx, row in df.iterrows():
                status_text.text(f"–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ {idx + 1} –∑ {len(df)}: {row.get('–ü–∏—Ç–∞–Ω–Ω—è UKR', 'N/A')}")

                doc_url = row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∞–Ω–æ—Ç–∞—Ü—ñ—é, –∞–Ω–∫–æ—Ä', '')
                base_page_url = row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å–∞–π—Ç—ñ', '') or row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è', '') or row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ', '') or row.get('URL', '') or ''

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ URL
                if not base_page_url or not (base_page_url.startswith('http://') or base_page_url.startswith('https://')):
                    st.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –∑–∞–ø–∏—Å {idx + 1} —á–µ—Ä–µ–∑ –Ω–µ–≤–∞–ª—ñ–¥–Ω–∏–π URL: {base_page_url}")
                    results.append({
                        '–ü–∏—Ç–∞–Ω–Ω—è UKR': row.get('–ü–∏—Ç–∞–Ω–Ω—è UKR', 'N/A'),
                        '–°—Ç–∞—Ç—É—Å': '–ü–æ–º–∏–ª–∫–∞',
                        '–î–µ—Ç–∞–ª—ñ': f"–ù–µ–≤–∞–ª—ñ–¥–Ω–∏–π –∞–±–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ–π URL: {base_page_url}",
                        'page_url_ru': base_page_url,
                        'page_url_ua': base_page_url,
                        'doc_url': doc_url,
                        'score_ru': 0,
                        'score_ua': 0,
                        'has_editor_ru': False,
                        'has_editor_ua': False,
                        'found_editors_ru': [],
                        'found_editors_ua': [],
                        'references_links_ru': [],
                        'references_links_ua': []
                    })
                    continue

                # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è URL –¥–ª—è –∫–æ–∂–Ω–æ—ó –ª–æ–∫–∞–ª—ñ
                page_url_ru = generate_russian_url(base_page_url)
                page_url_ua = generate_ukrainian_url(base_page_url)

                # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è —Ä–æ—Å—ñ–π—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
                doc_text = get_doc_text(doc_url)
                page_text_ru = get_page_text(page_url_ru)
                
                # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è" –¥–ª—è —Ä–æ—Å—ñ–π—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
                doc_fragment_ru = extract_annotation_fragment(doc_text)
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞: —è–∫—â–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π (–±—ñ–ª—å—à–µ 80% –≤—ñ–¥ –≤—Å—å–æ–≥–æ —Ç–µ–∫—Å—Ç—É),
                # –º–æ–∂–ª–∏–≤–æ –º–∞—Ä–∫–µ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —ñ –ø–æ–≤–µ—Ä–Ω—É—Ç–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                if doc_fragment_ru and len(doc_fragment_ru) > len(doc_text) * 0.8:
                    # –°–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è" –≤—Ä—É—á–Ω—É
                    annotation_match = re.search(r'–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è\s*:\s*(.*?)(?=\s*(?:–ê–Ω–∫–æ—Ä|–ü–µ—Ä–µ–≤–æ–¥|$))', doc_text, re.IGNORECASE | re.DOTALL)
                    if annotation_match:
                        doc_fragment_ru = annotation_match.group(1).strip()
                
                # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –ø–æ—Å–∏–ª–∞–Ω—å –∑ –±–ª–æ–∫—É "–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã" –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Å–∞–π—Ç—É (—Ä–æ—Å—ñ–π—Å—å–∫–∞)
                references_links_ru = extract_references_section(page_url_ru)
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ä–µ–¥–∞–∫—Ç–æ—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ (—Ä–æ—Å—ñ–π—Å—å–∫–∞)
                has_editor_ru, found_editors_ru = check_editors_on_page(page_text_ru)

                # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –¥–ª—è —Ä–æ—Å—ñ–π—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
                min_sim_ru, avg_sim_ru, missing_ru = compare_texts(
                    doc_fragment_ru, page_text_ru, threshold, chunk_size
                )

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
                page_text_ua = get_page_text(page_url_ua) if page_url_ua else ""
                
                # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ "–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏" –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
                doc_fragment_ua = extract_ukrainian_annotation_fragment(doc_text)
                
                # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –ø–æ—Å–∏–ª–∞–Ω—å –∑ –±–ª–æ–∫—É "–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã" –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Å–∞–π—Ç—É (—É–∫—Ä–∞—ó–Ω—Å—å–∫–∞)
                references_links_ua = extract_references_section(page_url_ua) if page_url_ua else []
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ä–µ–¥–∞–∫—Ç–æ—Ä—ñ–≤ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ (—É–∫—Ä–∞—ó–Ω—Å—å–∫–∞)
                has_editor_ua, found_editors_ua = check_editors_on_page(page_text_ua) if page_text_ua else (False, [])

                # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
                min_sim_ua, avg_sim_ua, missing_ua = compare_texts(
                    doc_fragment_ua, page_text_ua, threshold, chunk_size
                ) if doc_fragment_ua and page_text_ua else (0.0, 0.0, [])

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å—Å—ã–ª–∫–∏ ‚Äî –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –±—É–¥–µ–º –ø–æ–¥–≥—Ä—É–∂–∞—Ç—å –ø—Ä–∏ —Ä–∞—Å–∫—Ä—ã—Ç–∏–∏
                results.append({
                    'index': idx,
                    # –†–æ—Å—ñ–π—Å—å–∫–∞ –ª–æ–∫–∞–ª—å
                    'min_similarity_ru': round(min_sim_ru, 1),
                    'avg_similarity_ru': round(avg_sim_ru, 1),
                    'missing_count_ru': len(missing_ru),
                    'missing_fragments_ru': missing_ru,
                    'page_url_ru': page_url_ru,
                    'references_links_ru': references_links_ru,
                    'has_editor_ru': has_editor_ru,
                    'found_editors_ru': found_editors_ru,
                    # –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –ª–æ–∫–∞–ª—å
                    'min_similarity_ua': round(min_sim_ua, 1),
                    'avg_similarity_ua': round(avg_sim_ua, 1),
                    'missing_count_ua': len(missing_ua),
                    'missing_fragments_ua': missing_ua,
                    'page_url_ua': page_url_ua,
                    'references_links_ua': references_links_ua,
                    'has_editor_ua': has_editor_ua,
                    'found_editors_ua': found_editors_ua,
                    # –ó–∞–≥–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
                    'doc_preview': doc_text[:2000] if doc_text else "",
                    'page_preview_ru': page_text_ru[:2000] if page_text_ru else "",
                    'page_preview_ua': page_text_ua[:2000] if page_text_ua else "",
                    'doc_url': doc_url
                })

                progress_bar.progress((idx + 1) / len(df))
            
            st.session_state.results = results
            status_text.text("‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            progress_bar.empty()
    
    # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    if 'results' in st.session_state:
        st.markdown("---")
        st.header("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏")
        
        results_df = pd.DataFrame(st.session_state.results)
        # Map results by original index to allow safe lookup after filtering
        results_map = {r['index']: r for r in st.session_state.results}
        display_df = df.copy()
        
        # –†–æ—Å—ñ–π—Å—å–∫–∞ –ª–æ–∫–∞–ª—å
        display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å RU (%)'] = results_df['min_similarity_ru']
        display_df['–°–µ—Ä. —Å—Ö–æ–∂—ñ—Å—Ç—å RU (%)'] = results_df['avg_similarity_ru']
        display_df['–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ RU'] = results_df['missing_count_ru']
        
        # –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –ª–æ–∫–∞–ª—å
        display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å UA (%)'] = results_df['min_similarity_ua']
        display_df['–°–µ—Ä. —Å—Ö–æ–∂—ñ—Å—Ç—å UA (%)'] = results_df['avg_similarity_ua']
        display_df['–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ UA'] = results_df['missing_count_ua']
        
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å –∑ –æ–±–æ—Ö –ª–æ–∫–∞–ª–µ–π –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
        display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)'] = display_df[['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å RU (%)', '–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å UA (%)']].min(axis=1)
        
        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è
        if show_problems_only:
            display_df = display_df[display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)'] < threshold]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Ä–æ—Å—ñ–π—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
        st.markdown("### üá∑üá∫ –†–æ—Å—ñ–π—Å—å–∫–∞ –ª–æ–∫–∞–ª—å")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("–í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤", len(df))
        with col2:
            good_ru = len(display_df[display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å RU (%)'] >= threshold])
            st.metric("–î–æ–±—Ä–µ", good_ru, delta=f"{good_ru/len(df)*100:.0f}%")
        with col3:
            warning_ru = len(display_df[(display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å RU (%)'] >= threshold-15) & 
                                       (display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å RU (%)'] < threshold)])
            st.metric("–£–≤–∞–≥–∞", warning_ru, delta=f"{warning_ru/len(df)*100:.0f}%")
        with col4:
            problem_ru = len(display_df[display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å RU (%)'] < threshold-15])
            st.metric("–ü—Ä–æ–±–ª–µ–º–∞", problem_ru, delta=f"{problem_ru/len(df)*100:.0f}%")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
        st.markdown("### üá∫üá¶ –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –ª–æ–∫–∞–ª—å")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("–í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤", len(df))
        with col2:
            good_ua = len(display_df[display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å UA (%)'] >= threshold])
            st.metric("–î–æ–±—Ä–µ", good_ua, delta=f"{good_ua/len(df)*100:.0f}%")
        with col3:
            warning_ua = len(display_df[(display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å UA (%)'] >= threshold-15) & 
                                      (display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å UA (%)'] < threshold)])
            st.metric("–£–≤–∞–≥–∞", warning_ua, delta=f"{warning_ua/len(df)*100:.0f}%")
        with col4:
            problem_ua = len(display_df[display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å UA (%)'] < threshold-15])
            st.metric("–ü—Ä–æ–±–ª–µ–º–∞", problem_ua, delta=f"{problem_ua/len(df)*100:.0f}%")
        
        st.markdown("---")
        
        # –¢–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ - –æ–∫—Ä–µ–º—ñ –∑–∞–ø–∏—Å–∏ –¥–ª—è –∫–æ–∂–Ω–æ—ó –ª–æ–∫–∞–ª—ñ
        for idx, row in display_df.iterrows():
            result = results_map.get(idx, {})
            doc_url = row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∞–Ω–æ—Ç–∞—Ü—ñ—é, –∞–Ω–∫–æ—Ä', '')
            
            # –†–æ—Å—ñ–π—Å—å–∫–∞ –ª–æ–∫–∞–ª—å - –æ–∫—Ä–µ–º–∏–π –∑–∞–ø–∏—Å
            min_sim_ru = row['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å RU (%)']
            avg_sim_ru = row['–°–µ—Ä. —Å—Ö–æ–∂—ñ—Å—Ç—å RU (%)']
            
            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É –¥–ª—è —Ä–æ—Å—ñ–π—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
            if min_sim_ru >= threshold:
                status_icon_ru = "üü¢"
                status_color_ru = "green"
            elif min_sim_ru >= threshold - 15:
                status_icon_ru = "üü°"
                status_color_ru = "orange"
            else:
                status_icon_ru = "üî¥"
                status_color_ru = "red"
            
            with st.expander(
                f"{status_icon_ru} üá∑üá∫ **{row.get('–ü–∏—Ç–∞–Ω–Ω—è UKR', 'N/A')}** (RU) | –°—Ö–æ–∂—ñ—Å—Ç—å: {min_sim_ru}% | –§—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤: {row['–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ RU']}"
            ):
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    st.markdown(f"**–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å:** :{status_color_ru}[{min_sim_ru}%]")
                    st.markdown(f"**–°–µ—Ä–µ–¥–Ω—è —Å—Ö–æ–∂—ñ—Å—Ç—å:** {avg_sim_ru}%")
                    st.markdown(f"**–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤:** {row['–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ RU']}")
                
                with col2:
                    page_url_ru = result.get('page_url_ru', '')
                    if doc_url:
                        st.link_button("üìÑ –í—ñ–¥–∫—Ä–∏—Ç–∏ Google Doc", doc_url, use_container_width=True)
                    if page_url_ru:
                        st.link_button("üåê –í—ñ–¥–∫—Ä–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É", page_url_ru, use_container_width=True)
                
                # –ü—Ä–æ–±–ª–µ–º–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ (—Ä–æ—Å—ñ–π—Å—å–∫–∞)
                missing_fragments_ru = result.get('missing_fragments_ru', [])
                if missing_fragments_ru:
                    st.markdown("### ‚ö†Ô∏è –ô–º–æ–≤—ñ—Ä–Ω–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏:")
                    for i, fragment in enumerate(missing_fragments_ru, 1):
                        st.warning(f"**–§—Ä–∞–≥–º–µ–Ω—Ç {i}** (—Å—Ö–æ–∂—ñ—Å—Ç—å: {fragment['score']}%)\n\n{fragment['text']}")
                else:
                    st.success("‚úÖ –í—Å—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ!")
                
                # –°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã (—Ä–æ—Å—ñ–π—Å—å–∫–∞)
                references_links_ru = result.get('references_links_ru', [])
                if references_links_ru:
                    st.markdown("### üìö –°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã:")
                    for i, link in enumerate(references_links_ru, 1):
                        st.markdown(f"{i}. [{link}]({link})")
                else:
                    st.error("‚ùå –ü–æ–º–∏–ª–∫–∞: –ë–ª–æ–∫ '–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –ø–æ—Å–∏–ª–∞–Ω—å")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞ (—Ä–æ—Å—ñ–π—Å—å–∫–∞)
                has_editor_ru = result.get('has_editor_ru', False)
                found_editors_ru = result.get('found_editors_ru', [])
                if has_editor_ru and found_editors_ru:
                    editors_list_ru = ", ".join([f"**{editor}**" for editor in found_editors_ru])
                    st.success(f"‚úÖ –†–µ–¥–∞–∫—Ç–æ—Ä(–∏) –∑–Ω–∞–π–¥–µ–Ω–æ: {editors_list_ru}")
                else:
                    st.error("‚ùå –ü–æ–º–∏–ª–∫–∞: –†–µ–¥–∞–∫—Ç–æ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ (–æ—á—ñ–∫—É—î—Ç—å—Å—è: '–©–µ—Ä–±–∞—á–µ–Ω–∫–æ –Æ–ª—ñ—è' –∞–±–æ '–°–µ–≤—Ä—é–∫–æ–≤ –û–ª–µ–∫—Å–∞–Ω–¥—Ä –í—ñ–∫—Ç–æ—Ä–æ–≤–∏—á')")

                # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è ‚Äî –ø—ñ–¥–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø–æ–≤–Ω—ñ —Ç–µ–∫—Å—Ç–∏ –ø—Ä–∏ –≤—ñ–¥–∫—Ä–∏—Ç—Ç—ñ (–∫–µ—à—É—é—Ç—å—Å—è —Ñ—É–Ω–∫—Ü—ñ—è–º–∏)
                with st.expander("üîç –î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è"):
                    tab1, tab2 = st.tabs(["–¢–µ–∫—Å—Ç –∑ Docs", "–¢–µ–∫—Å—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏"])

                    with tab1:
                        if doc_url:
                            full_doc = get_doc_text(doc_url)
                            if full_doc:
                                display_text = full_doc if len(full_doc) <= 5000 else full_doc[:5000] + "..."
                                st.text_area(
                                    "–ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –∑ Google Docs",
                                    display_text,
                                    height=300,
                                    key=f"doc_text_ru_{idx}"
                                )
                            else:
                                st.info("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ–∫—Å—Ç –∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
                        else:
                            st.info("–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –≤–∫–∞–∑–∞–Ω–µ")

                    with tab2:
                        page_url_ru = result.get('page_url_ru', '')
                        if page_url_ru:
                            full_page_ru = get_page_text(page_url_ru)
                            if full_page_ru:
                                display_text = full_page_ru if len(full_page_ru) <= 5000 else full_page_ru[:5000] + "..."
                                st.text_area(
                                    "–ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏",
                                    display_text,
                                    height=300,
                                    key=f"page_text_ru_{idx}"
                                )
                            else:
                                st.info("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ–∫—Å—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏")
                        else:
                            st.info("–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –Ω–µ –≤–∫–∞–∑–∞–Ω–µ")
            
            # –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –ª–æ–∫–∞–ª—å - –æ–∫—Ä–µ–º–∏–π –∑–∞–ø–∏—Å
            min_sim_ua = row['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å UA (%)']
            avg_sim_ua = row['–°–µ—Ä. —Å—Ö–æ–∂—ñ—Å—Ç—å UA (%)']
            
            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –ª–æ–∫–∞–ª—ñ
            if min_sim_ua >= threshold:
                status_icon_ua = "üü¢"
                status_color_ua = "green"
            elif min_sim_ua >= threshold - 15:
                status_icon_ua = "üü°"
                status_color_ua = "orange"
            else:
                status_icon_ua = "üî¥"
                status_color_ua = "red"
            
            with st.expander(
                f"{status_icon_ua} üá∫üá¶ **{row.get('–ü–∏—Ç–∞–Ω–Ω—è UKR', 'N/A')}** (UA) | –°—Ö–æ–∂—ñ—Å—Ç—å: {min_sim_ua}% | –§—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤: {row['–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ UA']}"
            ):
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    st.markdown(f"**–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å:** :{status_color_ua}[{min_sim_ua}%]")
                    st.markdown(f"**–°–µ—Ä–µ–¥–Ω—è —Å—Ö–æ–∂—ñ—Å—Ç—å:** {avg_sim_ua}%")
                    st.markdown(f"**–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤:** {row['–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ UA']}")
                
                with col2:
                    page_url_ua = result.get('page_url_ua', '')
                    if doc_url:
                        st.link_button("üìÑ –í—ñ–¥–∫—Ä–∏—Ç–∏ Google Doc", doc_url, use_container_width=True)
                    if page_url_ua:
                        st.link_button("üåê –í—ñ–¥–∫—Ä–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É", page_url_ua, use_container_width=True)
                
                # –ü—Ä–æ–±–ª–µ–º–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ (—É–∫—Ä–∞—ó–Ω—Å—å–∫–∞)
                missing_fragments_ua = result.get('missing_fragments_ua', [])
                if missing_fragments_ua:
                    st.markdown("### ‚ö†Ô∏è –ô–º–æ–≤—ñ—Ä–Ω–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏:")
                    for i, fragment in enumerate(missing_fragments_ua, 1):
                        st.warning(f"**–§—Ä–∞–≥–º–µ–Ω—Ç {i}** (—Å—Ö–æ–∂—ñ—Å—Ç—å: {fragment['score']}%)\n\n{fragment['text']}")
                else:
                    if min_sim_ua > 0:  # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±—É–ª–∞ –≤–∏–∫–æ–Ω–∞–Ω–∞
                        st.success("‚úÖ –í—Å—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ!")
                    else:
                        st.info("‚ÑπÔ∏è –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–µ –≤–∏–∫–æ–Ω–∞–Ω–∞ (–º–æ–∂–ª–∏–≤–æ, –≤—ñ–¥—Å—É—Ç–Ω—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –≤–µ—Ä—Å—ñ—è –∞–±–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç '–ü–µ—Ä–µ–≤–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏')")
                
                # –°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã (—É–∫—Ä–∞—ó–Ω—Å—å–∫–∞)
                references_links_ua = result.get('references_links_ua', [])
                if references_links_ua:
                    st.markdown("### üìö –°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã:")
                    for i, link in enumerate(references_links_ua, 1):
                        st.markdown(f"{i}. [{link}]({link})")
                else:
                    if page_url_ua:
                        st.error("‚ùå –ü–æ–º–∏–ª–∫–∞: –ë–ª–æ–∫ '–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –ø–æ—Å–∏–ª–∞–Ω—å")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞ (—É–∫—Ä–∞—ó–Ω—Å—å–∫–∞)
                has_editor_ua = result.get('has_editor_ua', False)
                found_editors_ua = result.get('found_editors_ua', [])
                if has_editor_ua and found_editors_ua:
                    editors_list_ua = ", ".join([f"**{editor}**" for editor in found_editors_ua])
                    st.success(f"‚úÖ –†–µ–¥–∞–∫—Ç–æ—Ä(–∏) –∑–Ω–∞–π–¥–µ–Ω–æ: {editors_list_ua}")
                else:
                    if page_url_ua:
                        st.error("‚ùå –ü–æ–º–∏–ª–∫–∞: –†–µ–¥–∞–∫—Ç–æ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ (–æ—á—ñ–∫—É—î—Ç—å—Å—è: '–©–µ—Ä–±–∞—á–µ–Ω–∫–æ –Æ–ª—ñ—è' –∞–±–æ '–°–µ–≤—Ä—é–∫–æ–≤ –û–ª–µ–∫—Å–∞–Ω–¥—Ä –í—ñ–∫—Ç–æ—Ä–æ–≤–∏—á')")

                # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è ‚Äî –ø—ñ–¥–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø–æ–≤–Ω—ñ —Ç–µ–∫—Å—Ç–∏ –ø—Ä–∏ –≤—ñ–¥–∫—Ä–∏—Ç—Ç—ñ (–∫–µ—à—É—é—Ç—å—Å—è —Ñ—É–Ω–∫—Ü—ñ—è–º–∏)
                with st.expander("üîç –î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è"):
                    tab1, tab2 = st.tabs(["–¢–µ–∫—Å—Ç –∑ Docs", "–¢–µ–∫—Å—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏"])

                    with tab1:
                        if doc_url:
                            full_doc = get_doc_text(doc_url)
                            if full_doc:
                                display_text = full_doc if len(full_doc) <= 5000 else full_doc[:5000] + "..."
                                st.text_area(
                                    "–ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –∑ Google Docs",
                                    display_text,
                                    height=300,
                                    key=f"doc_text_ua_{idx}"
                                )
                            else:
                                st.info("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ–∫—Å—Ç –∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
                        else:
                            st.info("–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –≤–∫–∞–∑–∞–Ω–µ")

                    with tab2:
                        page_url_ua = result.get('page_url_ua', '')
                        if page_url_ua:
                            full_page_ua = get_page_text(page_url_ua)
                            if full_page_ua:
                                display_text = full_page_ua if len(full_page_ua) <= 5000 else full_page_ua[:5000] + "..."
                                st.text_area(
                                    "–ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏",
                                    display_text,
                                    height=300,
                                    key=f"page_text_ua_{idx}"
                                )
                            else:
                                st.info("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ–∫—Å—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏")
                        else:
                            st.info("–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –Ω–µ –≤–∫–∞–∑–∞–Ω–µ")

else:
    st.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Google Sheets.")

# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray;'>
    –†–æ–∑—Ä–æ–±–ª–µ–Ω–æ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ä–æ–∑–º—ñ—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤ –∑ Google Docs –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞—Ö —Å–∞–π—Ç—É
    </div>
    """,
    unsafe_allow_html=True
)