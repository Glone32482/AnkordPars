import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from rapidfuzz import fuzz
from google.oauth2 import service_account
from googleapiclient.discovery import build
import re
import gspread

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏
st.set_page_config(
    page_title="–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤",
    page_icon="üìÑ",
    layout="wide"
)

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è session state
if 'data' not in st.session_state:
    st.session_state.data = None
if 'selected_row' not in st.session_state:
    st.session_state.selected_row = None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è Google Docs API –∫–ª—ñ—î–Ω—Ç–∞
@st.cache_resource
def get_docs_service():
    try:
        credentials = service_account.Credentials.from_service_account_info(
            st.secrets["gcp_service_account"],
            scopes=["https://www.googleapis.com/auth/documents.readonly"]
        )
        service = build('docs', 'v1', credentials=credentials)
        return service
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Google Docs API: {e}")
        return None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ Google Sheets
@st.cache_data(ttl=600)
def load_data_from_sheets():
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º gspread + service account –∏–∑ st.secrets
        creds = service_account.Credentials.from_service_account_info(
            st.secrets["gcp_service_account"],
            scopes=["https://www.googleapis.com/auth/spreadsheets.readonly"]
        )

        client = gspread.Client(auth=creds)

        spreadsheet = st.secrets["connections"]["gsheets"]["spreadsheet"]
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ URL –∏–ª–∏ ID
        if isinstance(spreadsheet, str) and spreadsheet.startswith("http"):
            sh = client.open_by_url(spreadsheet)
        else:
            sh = client.open_by_key(spreadsheet)

        worksheet = sh.get_worksheet(0)
        records = worksheet.get_all_records()
        df = pd.DataFrame.from_records(records)

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ usecols=list(range(10))
        if df.shape[1] > 10:
            df = df.iloc[:, :10]

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = df.dropna(subset=['–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∞–Ω–æ—Ç–∞—Ü—ñ—é, –∞–Ω–∫–æ—Ä', '–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å–∞–π—Ç—ñ'])
        df = df.reset_index(drop=True)

        return df
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")
        return None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è document ID –∑ URL
def extract_doc_id(url):
    if not url or not isinstance(url, str):
        return None
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Å—Å—ã–ª–∫–∏: /d/<id>/, ?id=<id>, –∏ —Ç.–ø.
    match = re.search(r'(?:/d/|/document/d/|[?&]id=)([-\w]+)', url)
    return match.group(1) if match else None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ Google Docs
@st.cache_data(ttl=3600)
def get_doc_text(doc_url):
    if not doc_url:
        return ""
    
    doc_id = extract_doc_id(doc_url)
    if not doc_id:
        return ""
    
    try:
        service = get_docs_service()
        if not service:
            return ""
        
        document = service.documents().get(documentId=doc_id).execute()
        
        # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
        content = document.get('body', {}).get('content', [])
        text_parts = []
        
        for element in content:
            if 'paragraph' in element:
                paragraph = element['paragraph']
                for text_run in paragraph.get('elements', []):
                    if 'textRun' in text_run:
                        text_parts.append(text_run['textRun']['content'])
            elif 'table' in element:
                table = element['table']
                for row in table.get('tableRows', []):
                    for cell in row.get('tableCells', []):
                        for cell_content in cell.get('content', []):
                            if 'paragraph' in cell_content:
                                for text_run in cell_content['paragraph'].get('elements', []):
                                    if 'textRun' in text_run:
                                        text_parts.append(text_run['textRun']['content'])
        
        text = ''.join(text_parts)
        # –û—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    except Exception as e:
        st.warning(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {e}")
        return ""

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
@st.cache_data(ttl=3600)
def get_page_text(page_url):
    if not page_url:
        return ""
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å–∫—Ä–∏–ø—Ç—ñ–≤ —Ç–∞ —Å—Ç–∏–ª—ñ–≤
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        text = soup.get_text(separator='\n')
        # –û—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
        text = text.lower()
        text = text.replace('\xa0', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    except Exception as e:
        st.warning(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page_url}: {e}")
        return ""


# –û—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ Google Docs/Markdown-–ø–æ–¥—ñ–±–Ω–∏—Ö –≤–Ω–µ—Å–µ–Ω—å –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
def clean_google_docs_text(text: str) -> str:
    if not text or not isinstance(text, str):
        return ""

    # Replace non-breaking spaces and normalize newlines/spaces
    t = text.replace('\xa0', ' ')
    t = t.replace('\r\n', '\n').replace('\r', '\n')

    # Remove HTML tags
    t = re.sub(r'<[^>]+>', ' ', t)

    # Remove common Markdown-like separators and lines that contain only symbols (---, ***, ___)
    t = re.sub(r"(?m)^[\s\*\-_]{3,}$", ' ', t)

    # Remove bold/italic markers like **bold**, __italic__, *italic*
    t = re.sub(r'(\*\*|__)(.*?)\1', r'\2', t)
    t = re.sub(r'(?<!\*)\*(?!\*)', ' ', t)
    t = re.sub(r'_(.*?)_', r'\1', t)

    # Remove labels often used in the docs (with or without surrounding ** and with optional colon)
    labels = [
        '–ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç', '–ö–æ—Ä–æ—Ç–∫–∏–π –≤—ñ–¥–ø–æ–≤—ñ–¥—å', '–ö–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å', '–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è', '–ê–Ω–Ω–æ—Ç–∞—Ü—ñ—è', '–ê–Ω–∫–æ—Ä', '–ü–æ–¥–ø–∏—Å—å',
        '–ö–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–∑—é–º–µ', '–ö–æ—Ä–æ—Ç–∫–∞—è –≤–µ—Ä—Å–∏—è'
    ]
    for lab in labels:
        # remove patterns like **–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è:** or –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è:
        t = re.sub(rf"\*?\*?\s*{re.escape(lab)}\s*:?\*?\*?", ' ', t, flags=re.IGNORECASE)

    # Remove markdown headings (# Heading)
    t = re.sub(r'(?m)^\s{0,3}#+\s*', ' ', t)

    # Remove footnote markers like [1], [^1]
    t = re.sub(r'\[\^?.*?\]', ' ', t)

    # Remove multiple punctuation sequences often copied from editors
    t = re.sub(r'[\-\u2014]{2,}', ' ', t)
    t = re.sub(r'\.\.{2,}', ' ', t)

    # Remove leftover asterisks and underscores
    t = t.replace('*', ' ').replace('_', ' ')

    # Collapse whitespace and trim
    t = re.sub(r'\s+', ' ', t).strip()

    return t


# –í–∏—Ç—è–≥–Ω—É—Ç–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –º—ñ–∂ –º–∞—Ä–∫–µ—Ä–∞–º–∏ '–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è' —ñ '–ê–Ω–∫–æ—Ä'
def extract_annotation_fragment(text: str) -> str:
    if not text or not isinstance(text, str):
        return ""

    # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –º–∞—Ä–∫–µ—Ä–∏ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –ø–æ—à—É–∫—É
    t = text.replace('\xa0', ' ')
    t = re.sub(r'\s+', ' ', t)

    # –í–∞—Ä—ñ–∞–Ω—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ (–∫–∏—Ä–∏–ª–∏—Ü—è): –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è / –ê–Ω–Ω–æ—Ç–∞—Ü—ñ—è
    start_patterns = [r'–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è', r'–ê–Ω–Ω–æ—Ç–∞—Ü—ñ—è', r'–ê–Ω–Ω–æ—Ç–∞—Ü?—ñ?—è']
    end_patterns = [r'–ê–Ω–∫–æ—Ä', r'Anchor', r'–ê–Ω–∫o—Ä']

    # –ü–æ–±—É–¥—É—î–º–æ regex –¥–ª—è –ø–æ—à—É–∫—É –ø–æ–∑–∏—Ü—ñ–π (—ñ–≥–Ω–æ—Ä—É—é—á–∏ —Ä–µ–≥—ñ—Å—Ç—Ä —ñ –º–æ–∂–ª–∏–≤—ñ –¥–≤–æ–∫—Ä–∞–ø–∫–∏/–∑—ñ—Ä–æ—á–∫–∏)
    start_re = re.compile(r'(' + '|'.join(start_patterns) + r')\s*[:\-‚Äì‚Äî]*', flags=re.IGNORECASE)
    end_re = re.compile(r'(' + '|'.join(end_patterns) + r')\s*[:\-‚Äì‚Äî]*', flags=re.IGNORECASE)

    start_match = start_re.search(t)
    if not start_match:
        return t.strip()

    # –®—É–∫–∞—î–º–æ –ø–µ—Ä—à–∏–π –º–∞—Ç—á end –ø—ñ—Å–ª—è start
    start_pos = start_match.end()
    end_match = end_re.search(t, pos=start_pos)

    if end_match:
        fragment = t[start_pos:end_match.start()]
    else:
        # –Ø–∫—â–æ –µ–Ω–¥ –º–∞—Ä–∫–µ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî –±–µ—Ä–µ–º–æ —Ä–µ—à—Ç—É —Ç–µ–∫—Å—Ç—É –ø—ñ—Å–ª—è –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        fragment = t[start_pos:]

    # –û—á–∏—Å—Ç–∏–º–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤—ñ–¥ –∑–∞–π–≤–∏—Ö –∑—ñ—Ä–æ—á–æ–∫/–º–µ—Ç–æ–∫ —ñ –æ–±—Ä—ñ–∂–µ–º–æ
    fragment = re.sub(r'[\*_#]{1,}', ' ', fragment)
    fragment = re.sub(r'\s+', ' ', fragment).strip()

    return fragment

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤
def compare_texts(doc_text, page_text, threshold=75, chunk_size=300):
    if not doc_text or not page_text:
        return 0.0, 0.0, []

    # –û—á–∏—â–µ–Ω–Ω—è —ñ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    # –°–ø–æ—á–∞—Ç–∫—É –≤–∏—Ç—è–≥—É—î–º–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç '–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è' –∑ —Å–∏—Ä–æ–≥–æ —Ç–µ–∫—Å—Ç—É, —â–æ–± –º–∞—Ä–∫–µ—Ä–∏ –Ω–µ –±—É–ª–∏ –≤–∏–¥–∞–ª–µ–Ω—ñ
    doc_fragment_raw = extract_annotation_fragment(doc_text)

    # –û—á–∏—â–∞—î–º–æ –≤–∏—Ç—è–≥–Ω—É—Ç–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —ñ —Ç–µ–∫—Å—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –ø–µ—Ä–µ–¥ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è–º
    doc_text_clean = clean_google_docs_text(doc_fragment_raw)
    page_text_clean = clean_google_docs_text(page_text)

    doc_text_lower = doc_text_clean.lower()
    page_text_lower = page_text_clean.lower()

    # –†–æ–∑–±–∏—Ç—Ç—è –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ (–ø–æ —Å–ª–æ–≤–∞—Ö) –∑ –Ω–µ–≤–µ–ª–∏–∫–∏–º –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è–º
    words = doc_text_lower.split()
    if not words:
        return 0.0, 0.0, []

    chunks = []
    # –î–µ–ª–∞—é—Ç —à–∞–≥ —Ä–∞–≤–Ω—ã–π 80% —Ä–∞–∑–º–µ—Ä–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ -> 20% overlap
    step = max(1, int(chunk_size * 0.8))

    orig_words = doc_text.split()
    min_words_threshold = 10

    for i in range(0, len(words), step):
        chunk_words = words[i:i + chunk_size]
        if len(chunk_words) >= min_words_threshold:
            original = ' '.join(orig_words[i:i + chunk_size])
            normalized = ' '.join(chunk_words)
            chunks.append({
                'original': original,
                'normalized': normalized
            })

    if not chunks:
        return 0.0, 0.0, []

    # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Å—Ö–æ–∂–æ—Å—Ç—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
    scores = []
    missing_fragments = []

    for chunk in chunks:
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ token_set_ratio –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        score = fuzz.token_set_ratio(chunk['normalized'], page_text_lower)
        scores.append(score)

        if score < threshold:
            missing_fragments.append({
                'text': chunk['original'][:200] + '...' if len(chunk['original']) > 200 else chunk['original'],
                'score': score
            })

    min_score = min(scores) if scores else 0.0
    avg_score = sum(scores) / len(scores) if scores else 0.0

    return min_score, avg_score, missing_fragments

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.title("üìÑ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤ –∑ Google Docs")
st.markdown("---")

# –ë—ñ—á–Ω–∞ –ø–∞–Ω–µ–ª—å –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏
with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è")
    
    threshold = st.slider(
        "–ü–æ—Ä—ñ–≥ –ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ (%)",
        min_value=0,
        max_value=100,
        value=75,
        help="–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –≤—ñ–¥—Å–æ—Ç–æ–∫ —Å—Ö–æ–∂–æ—Å—Ç—ñ –¥–ª—è –≤–≤–∞–∂–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É —Ä–æ–∑–º—ñ—â–µ–Ω–∏–º"
    )
    
    chunk_size = st.slider(
        "–†–æ–∑–º—ñ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ (—Å–ª–æ–≤–∞)",
        min_value=100,
        max_value=500,
        value=300,
        step=50,
        help="–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤ —É –∫–æ–∂–Ω–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏"
    )
    
    show_problems_only = st.checkbox(
        "–ü–æ–∫–∞–∑–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ñ –∑–∞–ø–∏—Å–∏",
        value=False
    )
    
    st.markdown("---")
    
    # –ö–Ω–æ–ø–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
    if st.button("üîÑ –û–Ω–æ–≤–∏—Ç–∏ –¥–∞–Ω—ñ", use_container_width=True):
        st.cache_data.clear()
        st.session_state.data = None
        st.rerun()
    
    st.markdown("---")
    st.markdown("### üìä –õ–µ–≥–µ–Ω–¥–∞")
    st.markdown(f"""
    - üü¢ **–î–æ–±—Ä–µ**: ‚â• {threshold}%
    - üü° **–£–≤–∞–≥–∞**: {threshold-15}‚Äì{threshold-1}%
    - üî¥ **–ü—Ä–æ–±–ª–µ–º–∞**: < {threshold-15}%
    """)

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
if st.session_state.data is None:
    with st.spinner("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ Google Sheets..."):
        st.session_state.data = load_data_from_sheets()

df = st.session_state.data

if df is not None and not df.empty:
    st.success(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤")
    
    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫—É –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –≤—Å—ñ—Ö –∑–∞–ø–∏—Å—ñ–≤", type="primary", use_container_width=True):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            results = []
            for idx, row in df.iterrows():
                status_text.text(f"–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ {idx + 1} –∑ {len(df)}: {row.get('–ü–∏—Ç–∞–Ω–Ω—è UKR', 'N/A')}")

                doc_url = row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∞–Ω–æ—Ç–∞—Ü—ñ—é, –∞–Ω–∫–æ—Ä', '')
                # –ü–æ–ø—ã—Ç–∫–∏ –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫—É –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                page_url = row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å–∞–π—Ç—ñ', '') or row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è', '') or row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ', '') or row.get('URL', '') or ''

                # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤
                doc_text = get_doc_text(doc_url)
                page_text = get_page_text(page_url)

                # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
                min_sim, avg_sim, missing = compare_texts(
                    doc_text, page_text, threshold, chunk_size
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å—Å—ã–ª–∫–∏ ‚Äî –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –±—É–¥–µ–º –ø–æ–¥–≥—Ä—É–∂–∞—Ç—å –ø—Ä–∏ —Ä–∞—Å–∫—Ä—ã—Ç–∏–∏
                results.append({
                    'index': idx,
                    'min_similarity': round(min_sim, 1),
                    'avg_similarity': round(avg_sim, 1),
                    'missing_count': len(missing),
                    'missing_fragments': missing,
                    'doc_preview': doc_text[:2000] if doc_text else "",
                    'page_preview': page_text[:2000] if page_text else "",
                    'doc_url': doc_url,
                    'page_url': page_url
                })

                progress_bar.progress((idx + 1) / len(df))
            
            st.session_state.results = results
            status_text.text("‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            progress_bar.empty()
    
    # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    if 'results' in st.session_state:
        st.markdown("---")
        st.header("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏")
        
        results_df = pd.DataFrame(st.session_state.results)
        # Map results by original index to allow safe lookup after filtering
        results_map = {r['index']: r for r in st.session_state.results}
        display_df = df.copy()
        display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)'] = results_df['min_similarity']
        display_df['–°–µ—Ä. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)'] = results_df['avg_similarity']
        display_df['–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤'] = results_df['missing_count']
        
        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è
        if show_problems_only:
            display_df = display_df[display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)'] < threshold]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("–í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤", len(df))
        with col2:
            good = len(display_df[display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)'] >= threshold])
            st.metric("–î–æ–±—Ä–µ", good, delta=f"{good/len(df)*100:.0f}%")
        with col3:
            warning = len(display_df[(display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)'] >= threshold-15) & 
                                    (display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)'] < threshold)])
            st.metric("–£–≤–∞–≥–∞", warning, delta=f"{warning/len(df)*100:.0f}%")
        with col4:
            problem = len(display_df[display_df['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)'] < threshold-15])
            st.metric("–ü—Ä–æ–±–ª–µ–º–∞", problem, delta=f"{problem/len(df)*100:.0f}%")
        
        st.markdown("---")
        
        # –¢–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        for idx, row in display_df.iterrows():
            min_sim = row['–ú—ñ–Ω. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)']
            avg_sim = row['–°–µ—Ä. —Å—Ö–æ–∂—ñ—Å—Ç—å (%)']
            
            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É
            if min_sim >= threshold:
                status_icon = "üü¢"
                status_color = "green"
            elif min_sim >= threshold - 15:
                status_icon = "üü°"
                status_color = "orange"
            else:
                status_icon = "üî¥"
                status_color = "red"
            
            with st.expander(
                f"{status_icon} **{row.get('–ü–∏—Ç–∞–Ω–Ω—è UKR', 'N/A')}** | –°—Ö–æ–∂—ñ—Å—Ç—å: {min_sim}% | –§—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤: {row['–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤']}"
            ):
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    st.markdown(f"**–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Å—Ö–æ–∂—ñ—Å—Ç—å:** :{status_color}[{min_sim}%]")
                    st.markdown(f"**–°–µ—Ä–µ–¥–Ω—è —Å—Ö–æ–∂—ñ—Å—Ç—å:** {avg_sim}%")
                    st.markdown(f"**–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤:** {row['–ü—Ä–æ–±–ª–µ–º–Ω–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤']}")
                
                with col2:
                    doc_url = row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∞–Ω–æ—Ç–∞—Ü—ñ—é, –∞–Ω–∫–æ—Ä', '')
                    page_url = row.get('–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å–∞–π—Ç—ñ', '')
                    
                    if doc_url:
                        st.link_button("üìÑ –í—ñ–¥–∫—Ä–∏—Ç–∏ Google Doc", doc_url, use_container_width=True)
                    if page_url:
                        st.link_button("üåê –í—ñ–¥–∫—Ä–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É", page_url, use_container_width=True)
                
                # –ü—Ä–æ–±–ª–µ–º–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏
                result = results_map.get(idx, {})
                if result.get('missing_fragments'):
                    st.markdown("### ‚ö†Ô∏è –ô–º–æ–≤—ñ—Ä–Ω–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏:")
                    for i, fragment in enumerate(result['missing_fragments'], 1):
                        st.warning(f"**–§—Ä–∞–≥–º–µ–Ω—Ç {i}** (—Å—Ö–æ–∂—ñ—Å—Ç—å: {fragment['score']}%)\n\n{fragment['text']}")
                else:
                    st.success("‚úÖ –í—Å—ñ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ!")

                # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è ‚Äî –ø—ñ–¥–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø–æ–≤–Ω—ñ —Ç–µ–∫—Å—Ç–∏ –ø—Ä–∏ –≤—ñ–¥–∫—Ä–∏—Ç—Ç—ñ (–∫–µ—à—É—é—Ç—å—Å—è —Ñ—É–Ω–∫—Ü—ñ—è–º–∏)
                with st.expander("üîç –î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è"):
                    tab1, tab2 = st.tabs(["–¢–µ–∫—Å—Ç –∑ Docs", "–¢–µ–∫—Å—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏"])

                    with tab1:
                        doc_url = result.get('doc_url', '')
                        if doc_url:
                            full_doc = get_doc_text(doc_url)
                            if full_doc:
                                display_text = full_doc if len(full_doc) <= 5000 else full_doc[:5000] + "..."
                                st.text_area(
                                    "–ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –∑ Google Docs",
                                    display_text,
                                    height=300
                                )
                            else:
                                st.info("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ–∫—Å—Ç –∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
                        else:
                            st.info("–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –≤–∫–∞–∑–∞–Ω–µ")

                    with tab2:
                        page_url = result.get('page_url', '')
                        if page_url:
                            full_page = get_page_text(page_url)
                            if full_page:
                                display_text = full_page if len(full_page) <= 5000 else full_page[:5000] + "..."
                                st.text_area(
                                    "–ü–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏",
                                    display_text,
                                    height=300
                                )
                            else:
                                st.info("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ–∫—Å—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏")
                        else:
                            st.info("–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –Ω–µ –≤–∫–∞–∑–∞–Ω–µ")

else:
    st.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Google Sheets.")

# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray;'>
    –†–æ–∑—Ä–æ–±–ª–µ–Ω–æ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ä–æ–∑–º—ñ—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—ñ–≤ –∑ Google Docs –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞—Ö —Å–∞–π—Ç—É
    </div>
    """,
    unsafe_allow_html=True
)